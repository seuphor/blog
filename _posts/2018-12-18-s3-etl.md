---
title: AWS S3 ETL in Python
date: 2018-12-18 23:10:00
categories:
- tutorials
tags:
- aws
- etl
---

How to load Data from AWS S3

## 1. Connect to S3 Bucket

First, you need aws_access_key and aws_secret_key to connect to S3. Please follow [this guide](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/) (*Creating IAM Users* tab) to get key and the secret.

Then you can connect to the S3 bucket:
```python
# imports
import boto # package which connect to S3
import tarfile # handle tarfile extration
import pandas as pd
from io import BytesIO # read Bytes type data
```

With the bucket_name and bucket_key, you can load the file from specific location:
```python
access_key = '12345'
secret_key = '54321'

s3_connection = boto.connect_s3(
    aws_access_key_id = access_key,
    aws_secret_access_key = secret_key
)

bucket = s3_connection.get_bucket('bucket_name', validate=False)

# get all keys in the bucket
keys = bucket.list()
keys = [k.name for k in keys]

# load the file
obj = bucket.get_key('bucket_key').get_contents_as_string()
```

### Load TarFile
```python
obj = BytesIO(obj)

# Reads a lookup file from an archive (fileobj)
def get_from_archive(fileobj, compressed_file):
    # Open the archive
    tarf = tarfile.open(fileobj=fileobj)
    print(tarf.getnames())
     
    # Get the file of interest
    compressed = tarf.extractfile(compressed_file)
    return compressed
 
 # in this case the file inside tarfile is called "search.log"
log = get_from_archive(obj, "search.log")

# you can read by line
lines = log.readlines()
for i,l in enumerate(lines):
    print(l.decode('utf8').split('\t'))

# or load into python dataframe
pd_df = pd.read_csv(log, sep='\t', header=None, encoding='utf8')
```